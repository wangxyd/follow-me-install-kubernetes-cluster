<!-- toc -->

tags: flanneld

# 05.部署 flannel 网络

kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472，**需要开放该端口**（如公有云 AWS 等）。

flaneel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建 `flannedl.1`（也可能是其它名称，如 flannel1 等） 接口。

flannel 将分配的 Pod 网段信息写入 `/run/flannel/docker` 文件，docker 后续使用这个文件中的环境变量设置 `docker0` 网桥。

## 下载和分发 flanneld 二进制文件

到 [https://github.com/coreos/flannel/releases](https://github.com/coreos/flannel/releases) 页面下载最新版本的发布包：

``` bash
[admin@k8s-admin ~]$ wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz
[admin@k8s-admin ~]$ mkdir flannel && tar -zxf flannel-v0.11.0-linux-amd64.tar.gz -C flannel
```

分发 flanneld 二进制文件到集群所有节点：

``` bash
[admin@k8s-admin ~]$ ansible k8s-workers -m copy -a "src=flannel/flanneld dest=/usr/local/bin/ mode=a+x"
[admin@k8s-admin ~]$ ansible k8s-workers -m copy -a "src=flannel/mk-docker-opts.sh dest=/usr/local/bin/ mode=a+x"
```

## 创建 flannel 证书和私钥

flannel 从 etcd 集群存取网段分配信息，而 etcd 集群启用了双向 x509 证书认证，所以需要为 flanneld 生成证书和私钥。

创建证书签名请求：

``` bash
[admin@k8s-admin ~]$ cat >cert/flanneld-csr.json <<EOF
{
    "CN": "flanneld",
    "hosts": [],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Jiangsu",
            "L": "Suzhou",
            "O": "k8s",
            "OU": "ityoudao"
        }
    ]
}
EOF
```

生成证书和私钥：

``` bash
[admin@k8s-admin ~]$ cfssl gencert \
  -ca=cert/ca.pem \
  -ca-key=cert/ca-key.pem \
  -config=cert/ca-config.json \
  -profile=kubernetes \
  cert/flanneld-csr.json | cfssljson -bare cert/flanneld
[admin@k8s-admin ~]$ ls cert/flannel*
cert/flanneld.csr  cert/flanneld-csr.json  cert/flanneld-key.pem  cert/flanneld.pem
```

将生成的证书和私钥分发到**所有节点**（master 和 worker）：

``` bash
[admin@k8s-admin ~]$ ansible k8s-workers -m copy -a "src=cert/flanneld-key.pem dest=/etc/flanneld/cert/"
[admin@k8s-admin ~]$ ansible k8s-workers -m copy -a "src=cert/flanneld.pem dest=/etc/flanneld/cert/"
```

## 向 etcd 写入集群 Pod 网段信息

注意：本步骤**只需执行一次**。

``` bash
[admin@k8s-admin ~]$ source environment.sh
[admin@k8s-admin ~]$ export ETCDCTL_API=2
[admin@k8s-admin ~]$ etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=cert/ca.pem \
  --cert-file=cert/flanneld.pem \
  --key-file=cert/flanneld-key.pem \
  set ${FLANNEL_ETCD_PREFIX}/config '{"Network":"'${CLUSTER_CIDR}'", "SubnetLen": 21, "Backend": {"Type": "vxlan"}}'
{"Network":"172.16.0.0/16", "SubnetLen": 21, "Backend": {"Type": "vxlan"}}
# 查看一下etcd中的数据
[admin@k8s-admin ~]$ etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=cert/ca.pem \
  --cert-file=cert/flanneld.pem \
  --key-file=cert/flanneld-key.pem \
  ls -r
/kubernetes
/kubernetes/network
/kubernetes/network/config
[admin@k8s-admin ~]$ etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=cert/ca.pem \
  --cert-file=cert/flanneld.pem \
  --key-file=cert/flanneld-key.pem \
  get /kubernetes/network/config
{"Network":"172.30.0.0/16", "SubnetLen": 21, "Backend": {"Type": "vxlan"}}
```
+ flanneld **当前版本 (v0.10.0) 不支持 etcd v3**，故使用 etcd v2 API 写入配置 key 和网段数据；
+ v2 和 v3 的 etcdctl 命令行参数不一样，比如 v2：ls/get，v3：get；v2：ca-file/cert-file/key-file，v3：cacert/cert/key。
+ 写入的 Pod 网段 ${CLUSTER_CIDR} 地址段如 /16 必须小于 SubnetLen，必须与 kube-controller-manager 的 `--cluster-cidr` 参数值一致；
+ 这里 Pod 网段网络地址为 16bit，SubnetLen 为 21bit，故可以容纳32（2^5）个Node节点。

## 创建 flanneld 的 systemd unit 文件

``` bash
[admin@k8s-admin ~]$ cat >systemd/flanneld.service << "EOF"
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/etc/flanneld/flanneld.conf
ExecStart=/usr/local/bin/flanneld $FLANNELD_ARGS
ExecStartPost=/usr/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=always
RestartSec=5
StartLimitInterval=0

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
[admin@k8s-admin ~]$ cat >systemd/flanneld.conf.j2 <<"EOF"
FLANNELD_ARGS=" \
  -etcd-cafile=/etc/kubernetes/cert/ca.pem \
  -etcd-certfile=/etc/flanneld/cert/flanneld.pem \
  -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \
  -etcd-endpoints={{ ETCD_ENDPOINTS }} \
  -etcd-prefix={{ FLANNEL_ETCD_PREFIX }} \
  -iface={{ IFACE }} \
  -ip-masq"
EOF
```
+ `mk-docker-opts.sh` 脚本将分配给 flanneld 的 Pod 子网网段信息写入 `/run/flannel/docker` 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；
+ flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 `-iface` 参数指定通信接口;
+ flanneld 运行时需要 root 权限；
+ `-ip-masq`: flanneld 为访问 Pod 网络外的流量设置 SNAT 规则，同时将传递给 Docker 的变量 `--ip-masq`（`/run/flannel/docker` 文件中）设置为 false，这样 Docker 将不再创建 SNAT 规则；
  Docker 的 `--ip-masq` 为 true 时，创建的 SNAT 规则比较“暴力”：将所有本节点 Pod 发起的、访问非 docker0 接口的请求做 SNAT，这样访问其他节点 Pod 的请求来源 IP 会被设置为 flannel.1 接口的 IP，导致目的 Pod 看不到真实的来源 Pod IP。
  flanneld 创建的 SNAT 规则比较温和，只对访问非 Pod 网段的请求做 SNAT。

完整 unit 见 [flanneld.service](https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/flanneld.service)

## 分发 flanneld systemd unit 文件到**所有节点**并启动 flanneld 服务
``` bash
[admin@k8s-admin ~]$ cat >systemd/flanneld.service.yaml <<"EOF"
- hosts: k8s-workers
  remote_user: root
  tasks:
  - name: copy flanneld.service
    copy:
      src: flanneld.service
      dest: /usr/lib/systemd/system/flanneld.service  
  - name: copy flanneld.conf
    template:
      src: flanneld.conf.j2
      dest: /etc/flanneld/flanneld.conf
  - name: enable and start flanneld.service
    systemd:
      name: flanneld
      state: restarted
      enabled: yes
      daemon_reload: yes
EOF
[admin@k8s-admin ~]$ ansible-playbook systemd/flanneld.service.yaml
[admin@k8s-admin ~]$ ansible k8s-workers -m shell -a "systemctl status flanneld.service|grep -e Loaded -e Active"
```
+ 确保状态为“active (running)”并且“enabled”。
+ 如果没有执行“etcdctl set /kubernetes/network/config”命令向etcd写入集群Pod网段信息，这里会报`Couldn't fetch network config: 100: Key not found (/kubernetes) [7]`错误。

## 检查分配给各 flanneld 的 Pod 网段信息
查看已分配的 Pod 子网段列表(/24):

``` bash
[admin@k8s-admin ~]$ etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=cert/ca.pem \
  --cert-file=cert/flanneld.pem \
  --key-file=cert/flanneld-key.pem \
  ls ${FLANNEL_ETCD_PREFIX}/subnets
```

输出（结果是部署情况而定，网段可能与下面不一样）：

``` bash
/kubernetes/network/subnets/172.16.128.0-21
/kubernetes/network/subnets/172.16.208.0-21
/kubernetes/network/subnets/172.16.216.0-21
/kubernetes/network/subnets/172.16.240.0-21
/kubernetes/network/subnets/172.16.88.0-21
```

查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址:

``` bash
[admin@k8s-admin ~]$ etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=cert/ca.pem \
  --cert-file=cert/flanneld.pem \
  --key-file=cert/flanneld-key.pem \
  get /kubernetes/network/subnets/172.16.128.0-21
```

输出（结果是部署情况而定，网段可能与下面不一样）：

``` bash
{"PublicIP":"192.168.99.105","BackendType":"vxlan","BackendData":{"VtepMAC":"5a:56:20:ff:34:95"}}
```

## 验证各节点能通过 Pod 网段互通

在 **各节点上部署** flannel 后，检查是否创建了 flannel 接口(名称可能为 flannel0、flannel.0、flannel.1 等)：

``` bash
[admin@k8s-admin ~]$ ansible k8s-workers -m shell -a "ip addr|grep flannel"
```

输出：
``` bash
k8s-node03 | CHANGED | rc=0 >>
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    inet 172.16.216.0/32 scope global flannel.1

k8s-node04 | CHANGED | rc=0 >>
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    inet 172.16.208.0/32 scope global flannel.1

k8s-node02 | CHANGED | rc=0 >>
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    inet 172.16.88.0/32 scope global flannel.1

k8s-node05 | CHANGED | rc=0 >>
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    inet 172.16.128.0/32 scope global flannel.1

k8s-node01 | CHANGED | rc=0 >>
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    inet 172.16.240.0/32 scope global flannel.1
```

在各节点上 ping 所有 flannel 接口 IP，确保能通。这里以 ping 节点 k8s-node05 为例：

``` bash
[admin@k8s-admin ~]$ ansible k8s-workers -m shell -a "ping -c 1 172.16.128.0"
k8s-node04 | CHANGED | rc=0 >>
PING 172.16.128.0 (172.16.128.0) 56(84) bytes of data.
64 bytes from 172.16.128.0: icmp_seq=1 ttl=64 time=0.669 ms

--- 172.16.128.0 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.669/0.669/0.669/0.000 ms

k8s-node02 | CHANGED | rc=0 >>
PING 172.16.128.0 (172.16.128.0) 56(84) bytes of data.
64 bytes from 172.16.128.0: icmp_seq=1 ttl=64 time=0.913 ms

--- 172.16.128.0 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.913/0.913/0.913/0.000 ms

k8s-node03 | CHANGED | rc=0 >>
PING 172.16.128.0 (172.16.128.0) 56(84) bytes of data.
64 bytes from 172.16.128.0: icmp_seq=1 ttl=64 time=0.633 ms

--- 172.16.128.0 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.633/0.633/0.633/0.000 ms

k8s-node01 | CHANGED | rc=0 >>
PING 172.16.128.0 (172.16.128.0) 56(84) bytes of data.
64 bytes from 172.16.128.0: icmp_seq=1 ttl=64 time=0.593 ms

--- 172.16.128.0 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.593/0.593/0.593/0.000 ms

k8s-node05 | CHANGED | rc=0 >>
PING 172.16.128.0 (172.16.128.0) 56(84) bytes of data.
64 bytes from 172.16.128.0: icmp_seq=1 ttl=64 time=0.126 ms

--- 172.16.128.0 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.126/0.126/0.126/0.000 ms
```
